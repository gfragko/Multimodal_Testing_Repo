{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning CLIP Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to c:\\users\\gfrag\\appdata\\local\\temp\\pip-req-build-nhye12s1\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: ftfy in c:\\python312\\lib\\site-packages (from clip==1.0) (6.3.1)\n",
      "Requirement already satisfied: packaging in c:\\python312\\lib\\site-packages (from clip==1.0) (24.0)\n",
      "Requirement already satisfied: regex in c:\\python312\\lib\\site-packages (from clip==1.0) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\python312\\lib\\site-packages (from clip==1.0) (4.66.4)\n",
      "Requirement already satisfied: torch in c:\\python312\\lib\\site-packages (from clip==1.0) (2.3.1+cu121)\n",
      "Requirement already satisfied: torchvision in c:\\python312\\lib\\site-packages (from clip==1.0) (0.18.1+cu121)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\gfrag\\appdata\\roaming\\python\\python312\\site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in c:\\python312\\lib\\site-packages (from torch->clip==1.0) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\python312\\lib\\site-packages (from torch->clip==1.0) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\python312\\lib\\site-packages (from torch->clip==1.0) (1.13.0)\n",
      "Requirement already satisfied: networkx in c:\\python312\\lib\\site-packages (from torch->clip==1.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\python312\\lib\\site-packages (from torch->clip==1.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\python312\\lib\\site-packages (from torch->clip==1.0) (2024.6.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\python312\\lib\\site-packages (from torch->clip==1.0) (2021.4.0)\n",
      "Requirement already satisfied: numpy in c:\\python312\\lib\\site-packages (from torchvision->clip==1.0) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\python312\\lib\\site-packages (from torchvision->clip==1.0) (10.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\gfrag\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->clip==1.0) (0.4.6)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->clip==1.0) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->clip==1.0) (2021.13.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python312\\lib\\site-packages (from jinja2->torch->clip==1.0) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\python312\\lib\\site-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
      "Building wheels for collected packages: clip\n",
      "  Building wheel for clip (pyproject.toml): started\n",
      "  Building wheel for clip (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369594 sha256=e44edeca6f2f45c29e3dd872afae3cfbae499facdcccc6dd8308bfab6e6884ce\n",
      "  Stored in directory: C:\\Users\\gfrag\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-m_sshree\\wheels\\35\\3e\\df\\3d24cbfb3b6a06f17a2bfd7d1138900d4365d9028aa8f6e92f\n",
      "Successfully built clip\n",
      "Installing collected packages: clip\n",
      "Successfully installed clip-1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~angchain (C:\\Python312\\Lib\\site-packages)\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git 'C:\\Users\\gfrag\\AppData\\Local\\Temp\\pip-req-build-nhye12s1'\n",
      "WARNING: Ignoring invalid distribution ~angchain (C:\\Python312\\Lib\\site-packages)\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "import clip\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define Dataset Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_data_dir = \"New Plant Diseases Dataset(Augmented)\"  # Update with your local path\n",
    "\n",
    "def get_image_paths_and_labels(base_dir):\n",
    "    classes = os.listdir(base_dir)\n",
    "    data = []\n",
    "    for label in classes:\n",
    "        class_dir = os.path.join(base_dir, label)\n",
    "        if os.path.isdir(class_dir):\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                img_path = os.path.join(class_dir, img_name)\n",
    "                data.append((img_path, label))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantDiseaseDataset(Dataset):\n",
    "    def __init__(self, image_label_list, preprocess):\n",
    "        self.data = image_label_list\n",
    "        self.preprocess = preprocess\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.preprocess(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load CLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_image_paths_and_labels(os.path.join(local_data_dir, \"train\"))\n",
    "dataset = PlantDiseaseDataset(data, preprocess)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Fine-tune CLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:5504: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(5):  # Fine-tune for 5 epochs\n",
    "    for images, labels in dataloader:\n",
    "        images = images.to(device)\n",
    "        text_inputs = clip.tokenize(labels).to(device)\n",
    "        \n",
    "        image_features = model.encode_image(images)\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "        \n",
    "        logits_per_image, logits_per_text = model(images, text_inputs)\n",
    "        labels = torch.arange(len(images)).to(device)\n",
    "        \n",
    "        loss = (loss_fn(logits_per_image, labels) + loss_fn(logits_per_text, labels)) / 2\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "print(\"Fine-tuning complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Save the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save(model.state_dict(), \"fine_tuned_clip.pth\")\n",
    "print(\"Fine-tuning complete! Model saved as fine_tuned_clip.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
